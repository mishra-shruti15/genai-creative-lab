{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1731a791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============= BASIC PROMPT ==============\n",
      "Large Language Models (LLMs) are a type of artificial intelligence that can understand and generate human-like language. Here's a simplified explanation:\n",
      "\n",
      "**What is a Language Model?**\n",
      "\n",
      "A language model is a computer program that tries to predict the next word in a sentence, given the context of the previous words. It's like trying to complete a sentence: \"The capital of France is _______.\"\n",
      "\n",
      "**How do LLMs work?**\n",
      "\n",
      "LLMs are trained on massive amounts of text data (think millions or billions of books, articles, and conversations). This training data helps the model learn patterns, relationships, and nuances of language.\n",
      "\n",
      "Here's what happens when you ask an LLM a question:\n",
      "\n",
      "1. **Input**: You give the model a prompt or question.\n",
      "2. **Processing**: The model analyzes the input, using its vast knowledge to identify relevant words, phrases, and concepts.\n",
      "3. **Prediction**: Based on this analysis, the model generates a response that tries to answer your question or complete the sentence.\n",
      "4. **Feedback**: If you provide feedback (e.g., \"That's correct!\" or \"Not quite...\"), the model uses this information to improve its understanding and generate better responses in the future.\n",
      "\n",
      "**What can LLMs do?**\n",
      "\n",
      "LLMs are incredibly versatile and can:\n",
      "\n",
      "* Answer questions on various topics\n",
      "* Generate text, such as articles, stories, or even entire books\n",
      "* Translate languages (e.g., from English to Spanish)\n",
      "* Summarize long texts into shorter versions\n",
      "* Offer suggestions or recommendations based on your input\n",
      "\n",
      "**Why are LLMs important?**\n",
      "\n",
      "LLMs have many potential applications, including:\n",
      "\n",
      "* Improving language translation and interpretation\n",
      "* Enhancing customer service chatbots and virtual assistants\n",
      "* Assisting in content creation, such as writing articles or generating ideas\n",
      "* Helping with research and analysis by summarizing large amounts of text data\n",
      "\n",
      "In summary, Large Language Models are powerful AI systems that can understand and generate human-like language. They're trained on massive amounts of text data and can perform various tasks, from answering questions to generating text.\n",
      "\n",
      "========== MULTI-VARIABLE PROMPT ==========\n",
      "Prompt engineering! It's an exciting field that has gained significant attention in recent years, especially with the rise of natural language processing (NLP) and artificial intelligence (AI). As a data scientist, I'm happy to dive into the world of prompt engineering and explain it in simple terms.\n",
      "\n",
      "**What is Prompt Engineering?**\n",
      "\n",
      "Prompt engineering is the process of designing and crafting specific inputs or prompts that elicit desired responses from AI models, such as language translation systems, chatbots, or question-answering systems. The goal is to create prompts that are clear, concise, and effective in achieving a specific outcome.\n",
      "\n",
      "**Why is Prompt Engineering Important?**\n",
      "\n",
      "In the past, AI models were often trained on large datasets with limited context or guidance. However, as AI becomes more sophisticated, it's essential to provide high-quality inputs to ensure accurate and relevant responses. Prompt engineering helps bridge this gap by:\n",
      "\n",
      "1. **Improving model performance**: By crafting effective prompts, you can significantly improve the accuracy and relevance of AI-generated responses.\n",
      "2. **Enhancing user experience**: Well-designed prompts can lead to more natural-sounding conversations, making interactions with AI systems feel more human-like.\n",
      "3. **Reducing ambiguity**: Clear prompts help reduce misunderstandings and misinterpretations, which is crucial in applications like customer service or healthcare.\n",
      "\n",
      "**Key Principles of Prompt Engineering**\n",
      "\n",
      "To create effective prompts, follow these guidelines:\n",
      "\n",
      "1. **Be specific**: Clearly define what you want the AI model to do or respond with.\n",
      "2. **Use natural language**: Craft prompts that resemble everyday language, making it easier for humans and AI models to understand each other.\n",
      "3. **Keep it concise**: Aim for brevity while still conveying your intended meaning.\n",
      "4. **Test and refine**: Iterate on your prompts based on feedback from users or model performance metrics.\n",
      "\n",
      "**Examples of Prompt Engineering in Action**\n",
      "\n",
      "1. **Language Translation**: Designing prompts that accurately convey the context and nuances of a sentence, such as \"Translate this text into Spanish: 'The weather is nice today.'\"\n",
      "2. **Chatbots**: Crafting prompts for customer service chatbots to handle common queries, like \"What are your store hours?\"\n",
      "3. **Question-Answering Systems**: Creating prompts that elicit specific answers from AI-powered Q&A systems, such as \"What is the capital of France?\"\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Prompt engineering is a crucial step in developing effective AI systems that can understand and respond to human input. By following best practices and iterating on your prompts, you can improve model performance, enhance user experience, and reduce ambiguity. As AI continues to evolve, prompt engineering will play an increasingly important role in shaping the future of human-AI interactions.\n",
      "\n",
      "========== FEW-SHOT PROMPT ==========\n",
      "NLP stands for Natural Language Processing.\n",
      "\n",
      "========== ROLE-BASED PROMPT ==========\n",
      "Prompt engineering is the process of designing and crafting specific language prompts to elicit desired responses from language models, such as conversational AI systems or natural language processing (NLP) algorithms. The goal is to create prompts that are clear, concise, and effective in generating accurate and relevant outputs.\n",
      "\n",
      "Here's an example:\n",
      "\n",
      "Suppose you want a language model to generate a short story about a character who discovers a hidden treasure on a deserted island. You might start with a vague prompt like: \"Write a story about someone finding something cool on an island.\"\n",
      "\n",
      "However, this prompt is too open-ended and may lead to a wide range of responses that aren't what you're looking for. To improve the quality of the output, you can try rephrasing the prompt using more specific language:\n",
      "\n",
      "* \"On a deserted island, a character stumbles upon a mysterious chest buried in the sand. What do they find inside?\"\n",
      "* \"A solo traveler on a tropical getaway discovers a hidden treasure while exploring a secluded cove. Describe their excitement and what they do next.\"\n",
      "* \"In a dense jungle, an adventurer uncovers a long-lost treasure map that leads them to a hidden island paradise. How do they react when they finally find the loot?\"\n",
      "\n",
      "By using more specific language and adding context, you're giving the language model a better understanding of what you want it to generate. This can lead to more accurate and relevant responses.\n",
      "\n",
      "Some key principles of prompt engineering include:\n",
      "\n",
      "1. Be clear and concise: Avoid ambiguity by using simple and direct language.\n",
      "2. Provide context: Give the language model enough information to understand the topic or scenario.\n",
      "3. Use specific language: Instead of vague terms, use precise words and phrases that convey your intended meaning.\n",
      "4. Limit scope: Focus on a specific aspect or theme to guide the response.\n",
      "5. Test and refine: Try out different prompts and evaluate their effectiveness in generating desired responses.\n",
      "\n",
      "By applying these principles, you can create effective prompts that help language models generate high-quality outputs that meet your needs.\n",
      "\n",
      "========== INSTRUCTION + FORMAT PROMPT ==========\n",
      "Here's an explanation of Prompt Templates:\n",
      "\n",
      "**Definition**\n",
      "\n",
      "Prompt templates are pre-defined structures or formats for generating prompts that can be used to elicit specific types of responses from language models, such as chatbots, virtual assistants, or other AI systems. These templates provide a framework for crafting effective and consistent prompts that can help improve the accuracy and relevance of the model's responses.\n",
      "\n",
      "**Example**\n",
      "\n",
      "Here's an example of a prompt template:\n",
      "\n",
      "\"Can you [ACTION] [OBJECT/ENTITY] to achieve [GOAL]?\"\n",
      "\n",
      "This template can be used to generate prompts like:\n",
      "\n",
      "* \"Can you summarize the main points of this article?\"\n",
      "* \"Can you translate this sentence from English to Spanish?\"\n",
      "* \"Can you recommend a book that helps me learn about machine learning?\"\n",
      "\n",
      "**Use case**\n",
      "\n",
      "Prompt templates are useful in various scenarios, such as:\n",
      "\n",
      "1. **Consistency**: Using prompt templates ensures consistency in the way prompts are generated, which can improve the overall quality and accuracy of the model's responses.\n",
      "2. **Efficiency**: By providing a structured format for generating prompts, template-based approaches can reduce the time and effort required to craft effective prompts.\n",
      "3. **Customization**: Prompt templates can be tailored to specific use cases or domains, allowing you to fine-tune the prompts to elicit more relevant and accurate responses from the model.\n",
      "4. **Scalability**: As your language model or AI system grows in complexity and scope, prompt templates can help ensure that the prompts remain effective and consistent across different scenarios.\n",
      "\n",
      "In summary, prompt templates provide a structured approach to generating prompts for language models, which can improve consistency, efficiency, customization, and scalability.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task 6: Prompting\n",
    "Implement different PromtTemplates and promt Techniques using langchain\n",
    "LLM: Local open-source model via Ollama (llama3)\n",
    "'''\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    FewShotPromptTemplate\n",
    ")\n",
    "\n",
    "# Create Local LLM\n",
    "llm = ChatOllama(\n",
    "    model = 'llama3',\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Basic prompt Template\n",
    "def basic_prompt():\n",
    "    print(\"\\n============= BASIC PROMPT ==============\")\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=['topic'],\n",
    "        template=\"Explain {topic} in simple words.\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({'topic':\"Large Language Models\"})\n",
    "    print(response.content)\n",
    "\n",
    "# =================================================\n",
    "# MULTI-VARIABLE PROMPT TEMPLATE\n",
    "# =================================================\n",
    "def multi_variable_prompt():\n",
    "    print(\"\\n========== MULTI-VARIABLE PROMPT ==========\")\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"role\", \"task\"],\n",
    "        template=\"You are a {role}. Your task is to {task}.\"\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"role\": \"data scientist\",\n",
    "            \"task\": \"explain prompt engineering\"\n",
    "        }\n",
    "    )\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# FEW-SHOT PROMPTING\n",
    "# =================================================\n",
    "def few_shot_prompt():\n",
    "    print(\"\\n========== FEW-SHOT PROMPT ==========\")\n",
    "\n",
    "    examples = [\n",
    "        {\n",
    "            \"question\": \"What is AI?\",\n",
    "            \"answer\": \"AI stands for Artificial Intelligence.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is ML?\",\n",
    "            \"answer\": \"ML stands for Machine Learning.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    example_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\", \"answer\"],\n",
    "        template=\"Q: {question}\\nA: {answer}\"\n",
    "    )\n",
    "\n",
    "    few_shot_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"Answer the following questions:\",\n",
    "        suffix=\"Q: {input}\\nA:\",\n",
    "        input_variables=[\"input\"]\n",
    "    )\n",
    "\n",
    "    chain = few_shot_prompt | llm\n",
    "    response = chain.invoke({\"input\": \"What is NLP?\"})\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# ROLE-BASED PROMPTING (SYSTEM + HUMAN)\n",
    "# =================================================\n",
    "def role_based_prompt():\n",
    "    print(\"\\n========== ROLE-BASED PROMPT ==========\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert AI tutor.\"),\n",
    "            (\"human\", \"Explain prompt engineering with an example.\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({})\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# =================================================\n",
    "# INSTRUCTION + FORMAT PROMPTING\n",
    "# =================================================\n",
    "def instruction_format_prompt():\n",
    "    print(\"\\n========== INSTRUCTION + FORMAT PROMPT ==========\")\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=(\n",
    "            \"Explain {topic}.\\n\"\n",
    "            \"Return the answer in this format:\\n\"\n",
    "            \"- Definition\\n\"\n",
    "            \"- Example\\n\"\n",
    "            \"- Use case\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"topic\": \"Prompt Templates\"})\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main Execution\n",
    "# -------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    basic_prompt()\n",
    "    multi_variable_prompt()\n",
    "    few_shot_prompt()\n",
    "    role_based_prompt()\n",
    "    instruction_format_prompt()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
